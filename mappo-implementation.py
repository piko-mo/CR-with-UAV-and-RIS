import tensorflow as tf
import numpy as np
import os


class LogStdLayer(tf.keras.layers.Layer):
    """å¯å­¦ä¹ çš„å¯¹æ•°æ ‡å‡†å·®å±‚ - æ·»åŠ ç¡¬çº¦æŸ"""

    def __init__(self, action_dim, initial_value=-0.5, min_log_std=-2.0, max_log_std=0.0, **kwargs):
        super(LogStdLayer, self).__init__(**kwargs)
        self.action_dim = action_dim
        self.initial_value = initial_value
        self.min_log_std = min_log_std
        self.max_log_std = max_log_std

    def build(self, input_shape):
        self.log_std = self.add_weight(
            name='log_std',
            shape=(self.action_dim,),
            initializer=tf.keras.initializers.Constant(self.initial_value),
            trainable=True
        )
        super(LogStdLayer, self).build(input_shape)

    def call(self, inputs):
        batch_size = tf.shape(inputs)[0]
        # åœ¨forwardæ—¶ç¡¬çº¦æŸlog_stdèŒƒå›´ï¼Œé˜²æ­¢æ— é™å¢é•¿
        clipped_log_std = tf.clip_by_value(self.log_std, self.min_log_std, self.max_log_std)
        log_std_batch = tf.tile(tf.expand_dims(clipped_log_std, 0), [batch_size, 1])
        return inputs, log_std_batch


class RolloutBuffer:
    """
    è½¨è¿¹ç¼“å†²åŒº - ä¿®å¤ç‰ˆ

    ã€ä¿®å¤ã€‘åˆ†åˆ«å­˜å‚¨ï¼š
    - raw_actions: æ¯ä¸ªagentçš„æœªç¼©æ”¾åŠ¨ä½œ (ç”¨äºè®¡ç®—log_prob)
    - scaled_actions: ç¼©æ”¾åçš„è”åˆåŠ¨ä½œ (ç”¨äºç¯å¢ƒäº¤äº’)
    - log_probs: æ¯ä¸ªagentçš„log_prob
    """

    def __init__(self, num_agents):
        self.num_agents = num_agents
        self.states = []
        self.raw_actions = []  # ã€æ–°å¢ã€‘å­˜å‚¨æœªç¼©æ”¾çš„åŸå§‹åŠ¨ä½œ
        self.scaled_actions = []  # å­˜å‚¨ç¼©æ”¾åçš„è”åˆåŠ¨ä½œ
        self.rewards = []
        self.values = []
        self.log_probs = []  # [[agent0_lp, agent1_lp, agent2_lp], ...]
        self.dones = []

    def add(self, state, raw_actions_list, scaled_action, reward, value, log_probs_list, done):
        """
        æ·»åŠ ä¸€æ­¥æ•°æ®

        å‚æ•°:
            state: å…¨å±€çŠ¶æ€
            raw_actions_list: æ¯ä¸ªagentçš„åŸå§‹åŠ¨ä½œåˆ—è¡¨ (æœªç¼©æ”¾)
            scaled_action: ç¼©æ”¾åçš„è”åˆåŠ¨ä½œ
            reward: å¥–åŠ±
            value: å€¼å‡½æ•°ä¼°è®¡
            log_probs_list: æ¯ä¸ªagentçš„log_probåˆ—è¡¨
            done: æ˜¯å¦ç»ˆæ­¢
        """
        self.states.append(state)
        self.raw_actions.append(raw_actions_list)  # å­˜å‚¨åŸå§‹åŠ¨ä½œ
        self.scaled_actions.append(scaled_action)
        self.rewards.append(reward)
        self.values.append(value)
        self.log_probs.append(log_probs_list)
        self.dones.append(done)

    def clear(self):
        self.states = []
        self.raw_actions = []
        self.scaled_actions = []
        self.rewards = []
        self.values = []
        self.log_probs = []
        self.dones = []

    def get_batch(self):
        return (
            np.array(self.states, dtype=np.float32),
            np.array(self.raw_actions, dtype=np.float32),  # shape: [T, num_agents, action_dim]
            np.array(self.scaled_actions, dtype=np.float32),
            np.array(self.rewards, dtype=np.float32),
            np.array(self.values, dtype=np.float32),
            np.array(self.log_probs, dtype=np.float32),  # shape: [T, num_agents]
            np.array(self.dones, dtype=np.float32)
        )

    def size(self):
        return len(self.states)


class MAPPO:
    def __init__(self, env, num_agents=3, max_episodes=500, max_steps=500):
        # è¶…å‚æ•°
        self.gamma = 0.99
        self.gae_lambda = 0.95
        self.clip_ratio = 0.1  # ã€PPOæ ¸å¿ƒè£å‰ªå‚æ•°ã€‘
        self.actor_lr = 5e-4
        self.critic_lr = 5e-4
        self.update_epochs = 10
        self.mini_batch_size = 64
        self.entropy_coef = 0.001  # ç†µç³»æ•°
        self.value_loss_coef = 0.5
        self.max_grad_norm = 0.5

        # è®­ç»ƒå‚æ•°
        self.max_episodes = max_episodes
        self.max_steps = max_steps
        self.update_interval = max_steps

        self.env = env
        self.num_agents = num_agents
        self.state_dim = self.env.n_features
        self.action_dim = self.env.n_actions

        # åŠ¨ä½œè¾¹ç•Œ
        self.action_bounds = {
            'uav_height': (0, float(self.env.H_max)),
            'phase': (0, float(2 * np.pi)),
            'power': float(self.env.P_S_max)
        }

        # å†å²è®°å½•
        self.height_history = []
        self.power_history = []
        self.phase_history = []

        # åˆ›å»ºç½‘ç»œ
        self.actors = [self._build_actor() for _ in range(num_agents)]
        self.critic = self._build_critic()

        # ä¼˜åŒ–å™¨
        self.actor_optimizers = [
            tf.keras.optimizers.Adam(learning_rate=self.actor_lr)
            for _ in range(num_agents)
        ]
        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=self.critic_lr)

        # ä½¿ç”¨ä¿®å¤åçš„RolloutBuffer
        self.rollout_buffer = RolloutBuffer(num_agents)

        # å¥–åŠ±å½’ä¸€åŒ– - ä½¿ç”¨running mean/std
        self.reward_mean = 0
        self.reward_var = 1
        self.reward_count = 0
        self.warmup_steps = 100  # ã€æ–°å¢ã€‘é¢„çƒ­æ­¥æ•°ï¼Œå‰Næ­¥ä¸è¿›è¡Œå½’ä¸€åŒ–

    def _build_actor(self):
        """æ„å»ºActorç½‘ç»œ"""
        inputs = tf.keras.Input(shape=(self.state_dim,))
        x = tf.keras.layers.Dense(256, activation='relu',
                                  kernel_initializer='orthogonal')(inputs)
        x = tf.keras.layers.LayerNormalization()(x)
        x = tf.keras.layers.Dense(256, activation='relu',
                                  kernel_initializer='orthogonal')(x)
        x = tf.keras.layers.LayerNormalization()(x)
        x = tf.keras.layers.Dense(128, activation='relu',
                                  kernel_initializer='orthogonal')(x)

        # è¾“å‡ºå‡å€¼ï¼Œä½¿ç”¨æ›´å°çš„åˆå§‹åŒ–
        mu = tf.keras.layers.Dense(
            self.action_dim,
            activation='tanh',
            kernel_initializer=tf.keras.initializers.Orthogonal(gain=0.01),
            name='mu'
        )(x)

        # log_stdå±‚æ·»åŠ ç¡¬çº¦æŸ
        mu, log_std = LogStdLayer(
            self.action_dim,
            initial_value=-0.7,  # å¯¹åº” std â‰ˆ 0.5ï¼Œåˆå§‹æ¢ç´¢èŒƒå›´æ›´åˆç†
            min_log_std=-3.0,  # å…è®¸é«˜ç²¾åº¦å¾®è°ƒ
            max_log_std=-0.2
        )(mu)

        return tf.keras.Model(inputs=inputs, outputs=[mu, log_std])

    def _build_critic(self):
        """æ„å»ºCriticç½‘ç»œ"""
        inputs = tf.keras.Input(shape=(self.state_dim * self.num_agents,))
        x = tf.keras.layers.Dense(512, activation='relu',
                                  kernel_initializer='orthogonal')(inputs)
        x = tf.keras.layers.LayerNormalization()(x)
        x = tf.keras.layers.Dense(256, activation='relu',
                                  kernel_initializer='orthogonal')(x)
        x = tf.keras.layers.LayerNormalization()(x)
        x = tf.keras.layers.Dense(128, activation='relu',
                                  kernel_initializer='orthogonal')(x)
        value = tf.keras.layers.Dense(1, kernel_initializer=tf.keras.initializers.Orthogonal(gain=1.0))(x)
        return tf.keras.Model(inputs=inputs, outputs=value)

    def get_action_and_value(self, states, deterministic=False):
        """è·å–æ‰€æœ‰æ™ºèƒ½ä½“çš„åŠ¨ä½œã€å€¼å‡½æ•°å’Œå¯¹æ•°æ¦‚ç‡"""
        all_scaled_actions = []
        all_log_probs = []
        all_raw_actions = []

        for i in range(self.num_agents):
            state = np.expand_dims(states[i], axis=0).astype(np.float32)
            mu, log_std = self.actors[i](state)
            mu = mu.numpy()[0]
            log_std = log_std.numpy()[0]

            if not deterministic:
                std = np.exp(log_std)
                # ã€ä¿®å¤ã€‘å…ˆé‡‡æ ·ï¼Œå†è®¡ç®—log_probï¼Œæœ€åæ‰clip
                raw_action_unclipped = np.random.normal(mu, std)

                # åœ¨clipä¹‹å‰è®¡ç®—log_probï¼ˆè¿™æ˜¯æ­£ç¡®çš„æ¦‚ç‡å¯†åº¦ï¼‰
                log_prob = self._compute_log_prob(raw_action_unclipped, mu, std)

                # ç„¶åæ‰clipåŠ¨ä½œ
                raw_action = np.clip(raw_action_unclipped, -1.0, 1.0)
            else:
                raw_action = mu
                log_prob = 0.0

            all_raw_actions.append(raw_action)
            scaled_action = self._scale_action(raw_action)
            all_scaled_actions.append(scaled_action)
            all_log_probs.append(log_prob)

        # è®¡ç®—å€¼å‡½æ•°
        global_state = np.concatenate(states).reshape(1, -1).astype(np.float32)
        value = self.critic(global_state).numpy()[0, 0]

        return all_scaled_actions, all_log_probs, value, all_raw_actions

    def _compute_log_prob(self, action, mu, std):
        """è®¡ç®—é«˜æ–¯åˆ†å¸ƒçš„å¯¹æ•°æ¦‚ç‡"""
        var = std ** 2 + 1e-8
        log_prob = -0.5 * np.sum(
            ((action - mu) ** 2) / var +
            2 * np.log(std + 1e-8) + np.log(2 * np.pi)
        )
        return log_prob

    def _scale_action(self, raw_action):
        """å°†ç½‘ç»œè¾“å‡ºç¼©æ”¾åˆ°å®é™…åŠ¨ä½œèŒƒå›´"""
        scaled_action = []

        # UAVé«˜åº¦ç¼©æ”¾ [1, H_max]
        height_factor = (raw_action[0] + 1) / 2
        uav_height = 1.0 + height_factor * (self.action_bounds['uav_height'][1] - 1.0)
        scaled_action.append(uav_height)

        # ç›¸ä½ç¼©æ”¾ [0, 2Ï€]
        for i in range(self.env.N):
            phase = ((raw_action[i + 1] + 1) / 2) * 2 * np.pi
            scaled_action.append(phase)

        # åŠŸç‡ç¼©æ”¾ [0.1, P_S_max]
        min_power = 0.1
        power_ratio = (raw_action[-1] + 1) / 2
        power = min_power + power_ratio * (self.action_bounds['power'] - min_power)
        scaled_action.append(power)

        return np.array(scaled_action)

    def update_reward_stats(self, reward):
        """æ›´æ–°å¥–åŠ±ç»Ÿè®¡ï¼ˆWelfordç®—æ³•ï¼‰"""
        self.reward_count += 1
        delta = reward - self.reward_mean
        self.reward_mean += delta / self.reward_count
        delta2 = reward - self.reward_mean
        self.reward_var = ((self.reward_count - 1) * self.reward_var + delta * delta2) / max(self.reward_count, 1)
        self.reward_var = max(self.reward_var, 1e-6)

    def normalize_reward(self, reward):
        """å½’ä¸€åŒ–å¥–åŠ±"""
        # ã€ä¿®å¤ã€‘é¢„çƒ­æœŸé—´ä¸å½’ä¸€åŒ–
        if self.reward_count < self.warmup_steps:
            return reward
        return (reward - self.reward_mean) / (np.sqrt(self.reward_var) + 1e-8)

    def compute_gae(self, rewards, values, dones, last_value):
        """è®¡ç®—GAE"""
        advantages = np.zeros_like(rewards)
        last_gae = 0

        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = last_value
                next_non_terminal = 1.0 - dones[t]
            else:
                next_value = values[t + 1]
                next_non_terminal = 1.0 - dones[t]

            delta = rewards[t] + self.gamma * next_value * next_non_terminal - values[t]
            last_gae = delta + self.gamma * self.gae_lambda * next_non_terminal * last_gae
            advantages[t] = last_gae

        returns = advantages + values
        return advantages, returns

    def update(self, last_value):
        """PPOæ›´æ–° - ä¿®å¤ç‰ˆ"""
        (states, raw_actions, scaled_actions, rewards,
         values, old_log_probs, dones) = self.rollout_buffer.get_batch()
        # raw_actions shape: [T, num_agents, action_dim]
        # old_log_probs shape: [T, num_agents]

        # è®¡ç®—GAE
        advantages, returns = self.compute_gae(rewards, values, dones, last_value)

        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # åˆ†è§£å…¨å±€çŠ¶æ€ä¸ºå„æ™ºèƒ½ä½“çŠ¶æ€
        all_agents_states = []
        for i in range(self.num_agents):
            start_idx = i * self.state_dim
            end_idx = (i + 1) * self.state_dim
            all_agents_states.append(states[:, start_idx:end_idx])

        # å¤šä¸ªepochæ›´æ–°
        n_samples = len(states)
        indices = np.arange(n_samples)

        total_actor_loss = 0
        total_critic_loss = 0
        total_entropy = 0
        update_count = 0

        for epoch in range(self.update_epochs):
            np.random.shuffle(indices)

            for start in range(0, n_samples, self.mini_batch_size):
                end = min(start + self.mini_batch_size, n_samples)
                mb_indices = indices[start:end]

                mb_states = states[mb_indices]
                mb_advantages = advantages[mb_indices]
                mb_returns = returns[mb_indices]

                # æ›´æ–°Critic
                with tf.GradientTape() as tape:
                    value_preds = tf.reshape(self.critic(mb_states), [-1])
                    # ä½¿ç”¨Huber lossæ›´ç¨³å®š
                    critic_loss = tf.reduce_mean(tf.keras.losses.huber(mb_returns, value_preds))

                critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)
                critic_grads = [tf.clip_by_norm(g, self.max_grad_norm) for g in critic_grads]
                self.critic_optimizer.apply_gradients(
                    zip(critic_grads, self.critic.trainable_variables)
                )
                total_critic_loss += critic_loss.numpy()

                # æ›´æ–°å„Agentçš„Actor
                for i in range(self.num_agents):
                    mb_agent_states = all_agents_states[i][mb_indices]
                    # ã€ä¿®å¤ã€‘ç›´æ¥ä½¿ç”¨å­˜å‚¨çš„raw_actionsï¼Œæ— éœ€åå‘ç¼©æ”¾
                    mb_agent_actions = raw_actions[mb_indices, i, :]
                    mb_agent_old_log_probs = old_log_probs[mb_indices, i]

                    mb_agent_states = tf.convert_to_tensor(mb_agent_states, dtype=tf.float32)
                    mb_agent_actions = tf.convert_to_tensor(mb_agent_actions, dtype=tf.float32)

                    with tf.GradientTape() as tape:
                        mu, log_std = self.actors[i](mb_agent_states)
                        std = tf.exp(log_std)
                        var = std ** 2

                        # æ–°ç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡
                        new_log_probs = -0.5 * tf.reduce_sum(
                            ((mb_agent_actions - mu) ** 2) / (var + 1e-8) +
                            2 * tf.math.log(std + 1e-8) + np.log(2 * np.pi),
                            axis=-1
                        )

                        # è®¡ç®—ratio
                        mb_agent_old_log_probs_tensor = tf.convert_to_tensor(
                            mb_agent_old_log_probs, dtype=tf.float32
                        )
                        ratio = tf.exp(new_log_probs - mb_agent_old_log_probs_tensor)

                        # é¢å¤–clip ratioé˜²æ­¢æç«¯å€¼
                        ratio = tf.clip_by_value(ratio, 0.0, 10.0)

                        # ã€PPOæ ¸å¿ƒè£å‰ªã€‘
                        mb_advantages_tensor = tf.convert_to_tensor(mb_advantages, dtype=tf.float32)
                        surr1 = ratio * mb_advantages_tensor
                        surr2 = tf.clip_by_value(
                            ratio,
                            1 - self.clip_ratio,
                            1 + self.clip_ratio
                        ) * mb_advantages_tensor

                        # ç­–ç•¥æŸå¤±ï¼ˆå–minå®ç°æ‚²è§‚æ›´æ–°ï¼‰
                        policy_loss = -tf.reduce_mean(tf.minimum(surr1, surr2))

                        # ç†µ (entropy bonus) - é«˜æ–¯åˆ†å¸ƒç†µå…¬å¼
                        entropy = 0.5 * tf.reduce_mean(
                            tf.reduce_sum(tf.math.log(2 * np.pi * np.e * var + 1e-8), axis=-1)
                        )

                        # ã€ä¿®å¤ã€‘Actoræ€»æŸå¤± = ç­–ç•¥æŸå¤± - ç†µbonus
                        # æœ€å°åŒ–policy_lossï¼Œæœ€å¤§åŒ–entropy
                        actor_loss = policy_loss - self.entropy_coef * entropy

                    actor_grads = tape.gradient(actor_loss, self.actors[i].trainable_variables)
                    actor_grads = [tf.clip_by_norm(g, self.max_grad_norm) if g is not None else g
                                   for g in actor_grads]

                    grads_and_vars = [
                        (g, v) for g, v in zip(actor_grads, self.actors[i].trainable_variables)
                        if g is not None
                    ]
                    if grads_and_vars:
                        self.actor_optimizers[i].apply_gradients(grads_and_vars)

                    total_actor_loss += actor_loss.numpy()
                    total_entropy += entropy.numpy()

                update_count += 1

        # æ¸…ç©ºç¼“å†²åŒº
        self.rollout_buffer.clear()

        return (total_actor_loss / (update_count * self.num_agents),
                total_critic_loss / update_count,
                total_entropy / (update_count * self.num_agents))

    def train(self, print_freq=50):
        """è®­ç»ƒä¸»å¾ªç¯ - å·²ä¿®æ”¹ä¸ºæŒ‰ Episode æ›´æ–°"""
        reward_history = []
        rate_sum_history = []

        total_steps = 0
        best_avg_reward = -float('inf')

        for episode in range(self.max_episodes):
            states = self.env.reset()
            episode_rewards = []
            episode_rates = []

            step_heights = []
            step_powers = []
            step_phases = []

            print(f"\n====== å›åˆ {episode} ======")

            # --- æ­¥éª¤å¾ªç¯ (æ”¶é›†è½¨è¿¹) ---
            for step in range(self.max_steps):
                # 1. è·å–åŠ¨ä½œ
                all_scaled_actions, all_log_probs, value, all_raw_actions = self.get_action_and_value(states)

                # 2. ç»„åˆåŠ¨ä½œ
                uav_heights = [action[0] for action in all_scaled_actions]
                phases = [action[1:-1] for action in all_scaled_actions]
                powers = [action[-1] for action in all_scaled_actions]
                combined_action = np.concatenate((uav_heights, np.concatenate(phases), powers))

                # 3. ç¯å¢ƒäº¤äº’
                next_states, rewards, total_rate, agent_rates, C_P_list, penalty = self.env.step(combined_action)

                reward = rewards[0]
                # æ³¨æ„ï¼šå¯¹äºæ—¶é—´é™åˆ¶çš„ä»»åŠ¡ï¼Œæœ€åä¸€æ­¥é€šå¸¸ä¸ç®—çœŸæ­£çš„ terminalï¼Œä½†ä¸ºäº†ä»£ç å…¼å®¹æ€§ä¿æŒ done=1
                done = 1.0 if step == self.max_steps - 1 else 0.0

                # 4. ç»Ÿè®¡ä¸å½’ä¸€åŒ–
                self.update_reward_stats(reward)
                normalized_reward = self.normalize_reward(reward)

                # 5. å­˜å…¥ Buffer
                global_state = np.concatenate(states)
                self.rollout_buffer.add(
                    global_state,
                    all_raw_actions,
                    combined_action,
                    normalized_reward,
                    value,
                    all_log_probs,
                    done
                )

                # è®°å½•æ•°æ®
                episode_rewards.append(reward)
                episode_rates.append(total_rate)
                step_heights.append(uav_heights)
                step_powers.append(powers)
                step_phases.append(phases)
                total_steps += 1

                # æ‰“å°æ—¥å¿—
                if step % print_freq == 0:
                    print(f"\n----- æ­¥éª¤ {step} -----")
                    print(f" PUé€Ÿç‡: {C_P_list[0]:.4f}")
                    print(f" SUé€Ÿç‡: [{', '.join(f'{rate:.2f}' for rate in agent_rates)}]")
                    print(f" æ€»é€Ÿç‡: {total_rate:.4f}, å¥–åŠ±: {reward:.2f}")
                    # æ˜¾ç¤ºstdç›‘æ§
                    try:
                        _, log_std = self.actors[0](np.expand_dims(states[0], 0).astype(np.float32))
                        std_mean = np.mean(np.exp(log_std.numpy()))
                        print(f" stdå‡å€¼: {std_mean:.4f}")
                    except:
                        pass

                # çŠ¶æ€æµè½¬
                states = next_states

            # --- æ­¥éª¤å¾ªç¯ç»“æŸ ---

            # === ã€å…³é”®ä¿®æ”¹ã€‘åœ¨ Episode ç»“æŸåç»Ÿä¸€æ›´æ–° ===

            # 1. è®¡ç®—è¿™ä¸€å›åˆæœ€ç»ˆçŠ¶æ€çš„ Value (ç”¨äº GAE Bootstrap)
            # è¿™é‡Œçš„ states å·²ç»æ˜¯ next_states (å³ç¬¬300æ­¥ä¹‹åçš„çŠ¶æ€)
            global_state = np.concatenate(states).reshape(1, -1).astype(np.float32)
            last_value = self.critic(global_state).numpy()[0, 0]

            # 2. æ‰§è¡Œ PPO æ›´æ–°
            # è¿™ä¼šåˆ©ç”¨ Buffer ä¸­å®Œæ•´çš„ 300 æ­¥æ•°æ®è¿›è¡Œè®­ç»ƒ
            actor_loss, critic_loss, entropy = self.update(last_value)

            print(
                f"\n>>> å›åˆç»“æŸæ›´æ–°: Actor Loss={actor_loss:.4f}, Critic Loss={critic_loss:.4f}, Entropy={entropy:.4f}")

            # 3. è®°å½•å†å²æ•°æ®
            avg_reward = np.mean(episode_rewards)
            avg_rate = np.mean(episode_rates)

            reward_history.append(avg_reward)
            rate_sum_history.append(avg_rate)
            self.height_history.append(np.mean(step_heights, axis=0))
            self.power_history.append(np.mean(step_powers, axis=0))
            self.phase_history.append(np.mean(step_phases, axis=0))

            # 4. ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_reward > best_avg_reward:
                best_avg_reward = avg_reward
                print(f" ğŸ‰ æ–°æœ€ä½³å¥–åŠ±: {best_avg_reward:.4f}")

            print("\n" + "=" * 50)
            print(f"ç¬¬ {episode} å›åˆå®Œæˆ")
            print(f" å¹³å‡å¥–åŠ±: {avg_reward:.4f}")
            print(f" å¹³å‡é€Ÿç‡: {avg_rate:.4f}")
            print("=" * 50)

        return reward_history, rate_sum_history, self.height_history, self.power_history, self.phase_history


def plot_results(env, mappo, reward_history, rate_history, height_history, power_history, phase_history,
                 save_path='figures'):
    """ç»˜åˆ¶è®­ç»ƒç»“æœ"""
    import matplotlib.pyplot as plt
    try:
        import seaborn as sns
        sns.set_theme()
    except:
        pass

    os.makedirs(save_path, exist_ok=True)

    # 1. å¥–åŠ±æ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(reward_history, 'b-', alpha=0.3, label='Episode Reward')
    window = min(50, len(reward_history) // 5) if len(reward_history) > 10 else 1
    if window > 1:
        moving_avg = np.convolve(reward_history, np.ones(window) / window, mode='valid')
        plt.plot(range(window - 1, len(reward_history)), moving_avg, 'b-', linewidth=2, label='Moving Average')
    plt.xlabel('Episode')
    plt.ylabel('Average Reward')
    plt.title('Training Rewards')
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.savefig(f'{save_path}/reward_history.png', dpi=150, bbox_inches='tight')
    plt.close()

    # 2. é€Ÿç‡æ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(rate_history, 'g-', alpha=0.3, label='Episode Rate')
    if window > 1:
        moving_avg = np.convolve(rate_history, np.ones(window) / window, mode='valid')
        plt.plot(range(window - 1, len(rate_history)), moving_avg, 'g-', linewidth=2, label='Moving Average')
    plt.xlabel('Episode')
    plt.ylabel('Average Rate (bps/Hz)')
    plt.title('Secondary User Rate Sum')
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.savefig(f'{save_path}/rate_history.png', dpi=150, bbox_inches='tight')
    plt.close()

    # 3. UAVé«˜åº¦å˜åŒ–
    plt.figure(figsize=(10, 6))
    height_history = np.array(height_history)
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
    for i in range(height_history.shape[1]):
        plt.plot(height_history[:, i], color=colors[i], label=f'UAV {i + 1}', alpha=0.7)
    plt.xlabel('Episode')
    plt.ylabel('UAV Height (m)')
    plt.title('UAV Height Changes During Training')
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.savefig(f'{save_path}/height_changes.png', dpi=150, bbox_inches='tight')
    plt.close()

    # 4. åŠŸç‡å˜åŒ–
    plt.figure(figsize=(10, 6))
    power_history = np.array(power_history)
    for i in range(power_history.shape[1]):
        plt.plot(power_history[:, i], color=colors[i], label=f'Agent {i + 1}', alpha=0.7)
    plt.xlabel('Episode')
    plt.ylabel('Power (W)')
    plt.title('Transmit Power Changes During Training')
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.savefig(f'{save_path}/power_changes.png', dpi=150, bbox_inches='tight')
    plt.close()

    print(f"ç»“æœå·²ä¿å­˜åˆ° {save_path}/")


if __name__ == "__main__":
    from environment import Cognitive_Radio

    # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
    env = Cognitive_Radio(N=10)
    mappo = MAPPO(env, max_episodes=500, max_steps=300)

    print("å¼€å§‹è®­ç»ƒ...")
    print(f"çŠ¶æ€ç»´åº¦: {env.n_features}")
    print(f"åŠ¨ä½œç»´åº¦: {env.n_actions}")
    print(f"PPOè£å‰ªæ¯”ç‡: {mappo.clip_ratio}")

    # è®­ç»ƒ
    reward_history, rate_history, height_history, power_history, phase_history = mappo.train(
        print_freq=50
    )

    # ç»˜åˆ¶ç»“æœ
    plot_results(env, mappo, reward_history, rate_history, height_history, power_history, phase_history)